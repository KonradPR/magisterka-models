{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1271dc",
   "metadata": {},
   "source": [
    "<h4>TextCNN</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecff269",
   "metadata": {},
   "source": [
    "<h5>Imports</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470aa940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 13:25:27.091262: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-16 13:25:27.368684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 13:25:28.508911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse, unquote_plus\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Input, Dropout\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score\n",
    "from typing import Callable\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.profiler import profiler_v2 as profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9975964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loda all payloads from txt files in dataset folder (where each payload is one line)\n",
    "def load_payloads(dataset_path: str) -> list:\n",
    "    payloads = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        with open(dataset_path + filename, 'r') as f:\n",
    "            payloads.extend(f.readlines())\n",
    "    return payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d237b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=['malicious','normal']\n",
    "for name in file_names:\n",
    "    data=[]\n",
    "    with jsonlines.open('dataset/'+name+'.txt') as reader:\n",
    "        for line in reader:\n",
    "            if line['request']['method'] == 'POST':\n",
    "                data.append({'request':{'method':'POST', 'uri':line['request']['uri'], 'body':line['request']['body'],'headers':line['request']['headers']},'metadata':line['metdata']})\n",
    "            else:\n",
    "                data.append({'request':{'method':'GET', 'uri':line['request']['uri'], 'headers':line['request']['headers']},'metadata':line['metdata']})\n",
    "    with jsonlines.open('dataset/'+name+'_clean.txt', mode='w') as writer:\n",
    "        writer.write_all(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c0852",
   "metadata": {},
   "source": [
    "<h4>Load and create datasets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c294e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 13:25:45.115058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:45.278638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:45.278688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:45.282013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:45.282083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:45.282112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:46.445349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:46.445419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:46.445428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-16 13:25:46.445456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-16 13:25:46.445487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-05-16 13:25:48.504720: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [211600]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a  / a / a-a  { \"a-a\": \"a-a\", \"a\": \"a / a,a / a a,a / a;a = n.n,a / a,a / a,* / *;a = n.n\", \"a-a\": \"a\", \"a-a\": \"a / n.n  ( a; a a a a n_n_n )  a / n.n  ( a, a a )  a / n.n.n.n a / n.n\", \"a\": \"a = x; a = x\" } ' 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 13:25:48.769888: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [211600]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a   / a / a - a   { \" a - a \" :   \" a - a \" ,   \" a \" :   \" a / a , a / a   a , a / a ; a = n . n , a / a , a / a , * / * ; a = n . n \" ,   \" a - a \" :   \" a \" ,   \" a - a \" :   \" a / n . n   ( a ;   a   a   a   a   n _ n _ n )   a / n . n   ( a ,   a   a )   a / n . n . n . n   a / n . n \" ,   \" a \" :   \" a = x ;   a = x \" } ' 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "file_names = ['dataset/malicious_clean.txt' ,'dataset/normal_clean.txt']\n",
    "data={}\n",
    "for file in file_names:\n",
    "    data[file]=[]\n",
    "    with jsonlines.open(file) as reader:\n",
    "        for line in reader:\n",
    "            if line['request']['method'] == 'POST':\n",
    "                data[file].append('POST'+' '+line['request']['uri']+' '+line['request']['body']+' '+json.dumps(line['request']['headers']))\n",
    "            else:\n",
    "                data[file].append('GET'+' '+line['request']['uri']+' '+json.dumps(line['request']['headers']))\n",
    "\n",
    "payloads = load_payloads('dataset/payloads/')      \n",
    "                \n",
    "normal = data[file_names[1]]\n",
    "malicious = data[file_names[0]]\n",
    "normal_part1 = normal[180000:]\n",
    "normal_part2 = normal[:180000]\n",
    "\n",
    "train_examples = normal_part2\n",
    "test_examples = normal_part1+malicious\n",
    "train_labels = [0] * len(train_examples)\n",
    "test_labels = [0]* len(normal_part1)\n",
    "test_labels.extend([1] * len(malicious))\n",
    "\n",
    "payload_iterator={}\n",
    "payload_iterator[0] = 0\n",
    "def get_next_payload():\n",
    "    payload_iterator[0] = (payload_iterator[0] + 1) % len(payloads)\n",
    "    return payloads[payload_iterator[0]]\n",
    "\n",
    "#create synthetic malicious data by adding payloads to normal data (PAYLOAD SHOULD BE injected at a space character )\n",
    "tmp = normal_part1.copy()\n",
    "np.random.shuffle(payloads)\n",
    "synthetic_malicious = []\n",
    "for i in range(len(tmp)):\n",
    "    payload = get_next_payload()\n",
    "    tmp[i] = tmp[i].split(' ') \n",
    "    tmp[i].insert(np.random.randint(0,len(tmp[i])), payload)\n",
    "    tmp[i] = ' '.join(tmp[i])\n",
    "    synthetic_malicious.append(tmp[i])\n",
    "\n",
    "\n",
    "# add shuffled malicious data to train set\n",
    "#malicious_shuffled = malicious.copy()  \n",
    "#np.random.shuffle(malicious_shuffled)\n",
    "#train_examples.extend(malicious_shuffled[:50000])\n",
    "#train_labels.extend([1] * len(malicious_shuffled[:50000]))\n",
    "\n",
    "train_examples.extend(synthetic_malicious)\n",
    "train_labels.extend([1] * len(synthetic_malicious))\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((list(map(lambda x: unquote_plus(x),train_examples)), train_labels))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((list(map(lambda x: unquote_plus(x),test_examples)), test_labels))\n",
    "\n",
    "#shuflee datasets with given radnom seed\n",
    "dataset_train = dataset_train.shuffle(400000, seed=42, reshuffle_each_iteration=False)\n",
    "dataset_test = dataset_test.shuffle(400000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "def preprocess_text(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[a-zA-Z]+\", \"a\")\n",
    "    text = tf.strings.regex_replace(text, \"[0-9]+\", \"n\")\n",
    "    text = tf.strings.regex_replace(text, \"(a|n){2,}\", \"x\")\n",
    "    text = tf.strings.regex_replace(text, '[^\\x00-\\x7F]+', '')\n",
    "    text = tf.strings.regex_replace(text, '(.)',  r'\\1 ')\n",
    "    return text, label\n",
    "\n",
    "def preprocess_text_substitution(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[a-zA-Z]+\", \"a\")\n",
    "    text = tf.strings.regex_replace(text, \"[0-9]+\", \"n\")\n",
    "    text = tf.strings.regex_replace(text, \"(a|n){2,}\", \"x\")\n",
    "    text = tf.strings.regex_replace(text, '[^\\x00-\\x7F]+', '')\n",
    "    punctuation = \"=?/(){}[]<>\"\n",
    "    for p in punctuation:\n",
    "        text = tf.strings.regex_replace(text, \"\\\\\" + p, \" \"+p+\" \")\n",
    "    return text, label\n",
    "\n",
    " \n",
    "# Map the preprocess function to the dataset\n",
    "train = dataset_train.map(preprocess_text)\n",
    "test = dataset_test.map(preprocess_text)\n",
    "\n",
    "train_sub = dataset_train.map(preprocess_text_substitution)\n",
    "test_sub = dataset_test.map(preprocess_text_substitution)\n",
    "\n",
    "for text, label in train_sub.take(1):\n",
    "    print(text.numpy(), label.numpy())\n",
    "\n",
    "for text, label in train.take(1):\n",
    "    print(text.numpy(), label.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9246d4",
   "metadata": {},
   "source": [
    "<h4>Vocabularies</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3e08cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 00:43:23.850876: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-23 00:53:16.487408: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=256,\n",
    "    pad_to_max_tokens=True,\n",
    "    standardize=\"lower\"\n",
    ")\n",
    "\n",
    "vectorization_sub = TextVectorization(\n",
    "    max_tokens=5000, \n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=128,\n",
    "    pad_to_max_tokens=True,\n",
    "    standardize=\"lower\"\n",
    ")\n",
    "\n",
    "vectorization.adapt(train.map(lambda x, y: x))\n",
    "vectorization_sub.adapt(train_sub.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ffdfb",
   "metadata": {},
   "source": [
    "<h4>Save and load vectorization from disk</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d89fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 13:25:49.104957: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-16 13:25:49.275249: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "#pickle.dump({'config': vectorization.get_config(),'weights': vectorization.get_weights()}, open(\"vectorization.pkl\", \"wb\"))\n",
    "\n",
    "#pickle.dump({'config': vectorization_sub.get_config(), 'weights': vectorization_sub.get_weights()}, open(\"vectorization_sub.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "from_disk = pickle.load(open(\"vectorization.pkl\", \"rb\"))\n",
    "vectorization = TextVectorization.from_config(from_disk['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "vectorization.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "vectorization.set_weights(from_disk['weights'])\n",
    "\n",
    "from_disk_sub = pickle.load(open(\"vectorization_sub.pkl\", \"rb\"))\n",
    "vectorization_sub = TextVectorization.from_config(from_disk_sub['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "vectorization_sub.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "vectorization_sub.set_weights(from_disk_sub['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cf1bf",
   "metadata": {},
   "source": [
    "<h4>CNN</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0531230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 13:25:52.582477: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-05-16 13:25:52.582562: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2023-05-16 13:25:52.583541: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1679] Profiler found 1 GPUs\n",
      "2023-05-16 13:25:52.610907: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:194] cuptiSubscribe: error 15: CUPTI_ERROR_NOT_INITIALIZED\n",
      "2023-05-16 13:25:52.610951: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.\n",
      "2023-05-16 13:25:52.610970: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1730] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \n",
      "2023-05-16 13:25:52.625415: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype resource\n",
      "\t [[{{node Placeholder/_9}}]]\n",
      "2023-05-16 13:25:56.242958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-16 13:25:58.354918: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-16 13:25:58.709455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-16 13:25:58.736749: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fdd880752d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-16 13:25:58.736797: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-05-16 13:25:58.773685: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-16 13:25:59.084652: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-16 13:25:59.178330: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6613/6613 [==============================] - 76s 10ms/step - loss: 0.0336\n",
      "Epoch 2/15\n",
      "6613/6613 [==============================] - 67s 10ms/step - loss: 0.0079\n",
      "Epoch 3/15\n",
      "6613/6613 [==============================] - 68s 10ms/step - loss: 0.0042\n",
      "Epoch 4/15\n",
      "6613/6613 [==============================] - 68s 10ms/step - loss: 0.0029\n",
      "Epoch 5/15\n",
      "2830/6613 [===========>..................] - ETA: 11:23 - loss: 0.0023"
     ]
    }
   ],
   "source": [
    "embedding_layer_sub = tf.keras.layers.Embedding(1000, 4)\n",
    "embedding_layer = tf.keras.layers.Embedding(5000, 4)\n",
    "flatten_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "\n",
    "\n",
    "def sub_processing(text, label):\n",
    "    return  vectorization_sub(text), label\n",
    "\n",
    "def processing(text, label):\n",
    "    return  vectorization(text), label\n",
    "\n",
    "train_auto_sub = train_sub.map(sub_processing)\n",
    "train_auto = train.map(processing)\n",
    "\n",
    "train_auto = train_auto.batch(32)\n",
    "train_auto_sub = train_auto_sub.batch(32)\n",
    "\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(256,))\n",
    "mod = embedding_layer(input_layer)\n",
    "mod =  layers.Conv1D(128, 16, activation='relu', padding='same')(mod)\n",
    "mod = layers.MaxPooling1D(16, padding='same')(mod)\n",
    "mod =  layers.Conv1D(128, 8, activation='relu', padding='same')(mod)\n",
    "mod = layers.MaxPooling1D(8, padding='same')(mod)\n",
    "mod = flatten_layer(mod)\n",
    "mod =  tf.keras.layers.Dense(128, activation='relu')(mod)\n",
    "mod =  tf.keras.layers.Dense(1, activation='sigmoid')(mod)\n",
    "\n",
    "cnn = tf.keras.models.Model(input_layer, mod)\n",
    "\n",
    "input_layer_sub = tf.keras.layers.Input(shape=(256,))\n",
    "mod_sub = embedding_layer_sub(input_layer_sub)\n",
    "mod_sub =  layers.Conv1D(128, 16, activation='relu', padding='same')(mod_sub)\n",
    "mod_sub = layers.MaxPooling1D(16, padding='same')(mod_sub)\n",
    "mod_sub =  layers.Conv1D(128, 8, activation='relu', padding='same')(mod_sub)\n",
    "mod_sub = layers.MaxPooling1D(8, padding='same')(mod_sub)\n",
    "mod_sub = flatten_layer(mod_sub)\n",
    "mod_sub =  tf.keras.layers.Dense(128, activation='relu')(mod_sub)\n",
    "mod_sub =  tf.keras.layers.Dense(1, activation='sigmoid')(mod_sub)\n",
    "\n",
    "cnn_sub = tf.keras.models.Model(input_layer, mod_sub)\n",
    "\n",
    "# Compile and train the model\n",
    "cnn.compile(optimizer='adam',  loss=losses.BinaryCrossentropy())\n",
    "cnn_sub.compile(optimizer='adam',  loss=losses.BinaryCrossentropy())\n",
    "\n",
    "\n",
    "profiler.start(logdir='cnn_train')\n",
    "cnn.fit(train_auto, epochs=15)\n",
    "profiler.stop()\n",
    "profiler.start(logdir='cnn_sub_train')\n",
    "cnn_sub.fit(train_auto, epochs=15)\n",
    "profiler.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f91fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn.fit(train_auto, epochs=10)\n",
    "cnn.save('cnn/my_model')\n",
    "cnn = tf.keras.models.load_model('cnn/my_model')\n",
    "\n",
    "cnn_sub.save('cnn_sub/my_model')\n",
    "cnn_sub = tf.keras.models.load_model('cnn_sub/my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9868826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 09:39:16.057311: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [117899]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3685/3685 [==============================] - 26s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_auto_sub = test_sub.map(sub_processing)\n",
    "test_auto_sub = test_auto_sub.batch(32)\n",
    "test_auto = test.map(processing)\n",
    "test_auto = test_auto.batch(32)\n",
    "reconsturctions_sub =  cnn_sub.predict(test_auto_sub)\n",
    "reconsturctions =  cnn.predict(test_auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c01d3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(map(lambda x: 0 if x[0]<0.5 else 1, list(reconsturctions)))\n",
    "pred_sub = list(map(lambda x: 0 if x[0]<0.5 else 1, list(reconsturctions_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "073354ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 09:39:43.284034: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [117899]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "actual = test_auto.map(lambda x,y:y)\n",
    "actual = np.concatenate(list(actual.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3daa35e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.886868256946997"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Metrics\")\n",
    "print(\"f1: \", f1_score(actual,pred))\n",
    "print(\"acc: \", accuracy_score(actual,pred))\n",
    "print(\"prec: \", precision_score(actual,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb2db988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Metrics sub\")\n",
    "print(\"f1: \", f1_score(actual,pred_sub))\n",
    "print(\"acc: \", accuracy_score(actual,pred_sub))\n",
    "print(\"prec: \", precision_score(actual,pred_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd905c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
