{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1271dc",
   "metadata": {},
   "source": [
    "<h4>Transfer Learning</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecff269",
   "metadata": {},
   "source": [
    "<h5>Imports</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "470aa940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse, unquote_plus\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Input, Dropout\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score\n",
    "from typing import Callable\n",
    "from sklearn import svm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.profiler import profiler_v2 as profiler\n",
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, precision_recall_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "751e81ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 19:43:26.105114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a  / a / a  { \"a-a\": \"a-a,a-a\", \"a\": \"a / a,a / a a,a / a;a = n.n,a / a,a / a,* / *;a = n.n\", \"a-a\": \"a;a = n.n, a;a = n.n, *;a = n.n\", \"a-a\": \"a / n.n  ( a a n.n; x; x )  a / n.n  ( a, a a )  a / n.n.n.n a / n.n a / n.n.n.n\", \"a\": \"a = x; a = x\" } ' 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 19:43:26.380897: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a   / a / a   { \" a - a \" :   \" a - a , a - a \" ,   \" a \" :   \" a / a , a / a   a , a / a ; a = n . n , a / a , a / a , * / * ; a = n . n \" ,   \" a - a \" :   \" a ; a = n . n ,   a ; a = n . n ,   * ; a = n . n \" ,   \" a - a \" :   \" a / n . n   ( a   a   n . n ;   x ;   x )   a / n . n   ( a ,   a   a )   a / n . n . n . n   a / n . n   a / n . n . n . n \" ,   \" a \" :   \" a = x ;   a = x \" } ' 0\n"
     ]
    }
   ],
   "source": [
    "file_names=['malicious','normal']\n",
    "for name in file_names:\n",
    "    data=[]\n",
    "    with jsonlines.open('dataset/'+name+'.txt') as reader:\n",
    "        for line in reader:\n",
    "            if line['request']['method'] == 'POST':\n",
    "                data.append({'request':{'method':'POST', 'uri':line['request']['uri'], 'body':line['request']['body'],'headers':line['request']['headers']},'metadata':line['metdata']})\n",
    "            else:\n",
    "                data.append({'request':{'method':'GET', 'uri':line['request']['uri'], 'headers':line['request']['headers']},'metadata':line['metdata']})\n",
    "    with jsonlines.open('dataset/'+name+'_clean.txt', mode='w') as writer:\n",
    "        writer.write_all(data)\n",
    "        \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "file_names = ['dataset/malicious_clean.txt' ,'dataset/normal_clean.txt']\n",
    "data={}\n",
    "for file in file_names:\n",
    "    data[file]=[]\n",
    "    with jsonlines.open(file) as reader:\n",
    "        for line in reader:\n",
    "            if line['request']['method'] == 'POST':\n",
    "                data[file].append('POST'+' '+line['request']['uri']+' '+line['request']['body']+' '+json.dumps(line['request']['headers']))\n",
    "            else:\n",
    "                data[file].append('GET'+' '+line['request']['uri']+' '+json.dumps(line['request']['headers']))\n",
    "\n",
    "   \n",
    "                \n",
    "normal = data[file_names[1]]\n",
    "malicious = data[file_names[0]]\n",
    "normal_part1 = normal[180000:]\n",
    "normal_part2 = normal[:180000]\n",
    "\n",
    "train_examples = normal_part2\n",
    "test_examples = normal_part1+malicious\n",
    "train_labels = [0] * len(train_examples)\n",
    "test_labels = [0]* len(normal_part1)\n",
    "test_labels.extend([1] * len(malicious))\n",
    "\n",
    "\n",
    "\n",
    "# add shuffled malicious data to train set\n",
    "#malicious_shuffled = malicious.copy()  \n",
    "#np.random.shuffle(malicious_shuffled)\n",
    "#train_examples.extend(malicious_shuffled[:50000])\n",
    "#train_labels.extend([1] * len(malicious_shuffled[:50000]))\n",
    "\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((list(map(lambda x: unquote_plus(x),train_examples)), train_labels))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((list(map(lambda x: unquote_plus(x),test_examples)), test_labels))\n",
    "\n",
    "#shuflee datasets with given radnom seed\n",
    "dataset_train = dataset_train.shuffle(400000, seed=42, reshuffle_each_iteration=False)\n",
    "dataset_test = dataset_test.shuffle(400000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "def preprocess_text(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[a-zA-Z]+\", \"a\")\n",
    "    text = tf.strings.regex_replace(text, \"[0-9]+\", \"n\")\n",
    "    text = tf.strings.regex_replace(text, \"(a|n){2,}\", \"x\")\n",
    "    text = tf.strings.regex_replace(text, '[^\\x00-\\x7F]+', '')\n",
    "    text = tf.strings.regex_replace(text, '(.)',  r'\\1 ')\n",
    "    return text, label\n",
    "\n",
    "def preprocess_text_substitution(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[a-zA-Z]+\", \"a\")\n",
    "    text = tf.strings.regex_replace(text, \"[0-9]+\", \"n\")\n",
    "    text = tf.strings.regex_replace(text, \"(a|n){2,}\", \"x\")\n",
    "    text = tf.strings.regex_replace(text, '[^\\x00-\\x7F]+', '')\n",
    "    punctuation = \"=?/(){}[]<>\"\n",
    "    for p in punctuation:\n",
    "        text = tf.strings.regex_replace(text, \"\\\\\" + p, \" \"+p+\" \")\n",
    "    return text, label\n",
    "\n",
    " \n",
    "# Map the preprocess function to the dataset\n",
    "train = dataset_train.map(preprocess_text)\n",
    "test = dataset_test.map(preprocess_text)\n",
    "\n",
    "train_sub = dataset_train.map(preprocess_text_substitution)\n",
    "test_sub = dataset_test.map(preprocess_text_substitution)\n",
    "\n",
    "for text, label in train_sub.take(1):\n",
    "    print(text.numpy(), label.numpy())\n",
    "\n",
    "for text, label in train.take(1):\n",
    "    print(text.numpy(), label.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d237b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c74ffdfb",
   "metadata": {},
   "source": [
    "<h4>Save and load vectorization from disk</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d89fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from_disk = pickle.load(open(\"vectorization.pkl\", \"rb\"))\n",
    "vectorization = TextVectorization.from_config(from_disk['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "vectorization.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "vectorization.set_weights(from_disk['weights'])\n",
    "\n",
    "from_disk_sub = pickle.load(open(\"vectorization_sub.pkl\", \"rb\"))\n",
    "vectorization_sub = TextVectorization.from_config(from_disk_sub['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "vectorization_sub.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "vectorization_sub.set_weights(from_disk_sub['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cf1bf",
   "metadata": {},
   "source": [
    "<h4>Load and retrain all models</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0531230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load lstm model\n",
    "lstm_model = tf.keras.models.load_model('lstm/my_model')\n",
    "lstm_model_sub = tf.keras.models.load_model('lstm_sub/my_model')\n",
    "\n",
    "#load transformer model\n",
    "transformer_model = tf.keras.models.load_model('trans/my_model')\n",
    "transformer_model_sub = tf.keras.models.load_model('trans_sub/my_model')\n",
    "\n",
    "#load cnn model\n",
    "cnn_model = tf.keras.models.load_model('cnn/my_model') \n",
    "cnn_model_sub = tf.keras.models.load_model('cnn_sub/my_model') \n",
    "\n",
    "#load autoencoder model\n",
    "autoencoder_model = tf.keras.models.load_model('autoencoder/my_model')\n",
    "autoencoder_model_sub = tf.keras.models.load_model('autoencoder_sub/my_model')\n",
    "\n",
    "import pickle\n",
    "filename = './svm/svm_model.sav'\n",
    "svm_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "filename_sub = './svm_sub/svm_model.sav'\n",
    "svm_model_sub = pickle.load(open(filename_sub, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85f91fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 00:49:05.358997: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [117899]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "/tmp/ipykernel_6549/3238461473.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_auto_svm = np.array(list(test_auto.as_numpy_iterator()))\n",
      "2023-06-07 00:49:29.463953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype resource\n",
      "\t [[{{node Placeholder/_9}}]]\n",
      "/tmp/ipykernel_6549/3238461473.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_auto_sub_svm = np.array(list(test_auto_sub.as_numpy_iterator()))\n"
     ]
    }
   ],
   "source": [
    "#testdataset for normal_models:\n",
    "def sub_processing(text, label):\n",
    "    return  vectorization_sub(text), label\n",
    "\n",
    "def processing(text, label):\n",
    "    return  vectorization(text), label\n",
    "test_auto_sub = test_sub.map(sub_processing)\n",
    "\n",
    "test_auto = test.map(processing)\n",
    "\n",
    "\n",
    "\n",
    "#tesdataset for svm\n",
    "test_auto_svm = np.array(list(test_auto.as_numpy_iterator()))\n",
    "test_auto_sub_svm = np.array(list(test_auto_sub.as_numpy_iterator()))\n",
    "test_text = test_auto_svm[:,0]\n",
    "test_label = test_auto_svm[:,1]\n",
    "test_text_sub = test_auto_sub_svm[:,0]\n",
    "test_label_sub = test_auto_sub_svm[:,1]\n",
    "\n",
    "\n",
    "test_auto_sub = test_auto_sub.batch(32)\n",
    "test_auto = test_auto.batch(32)\n",
    "def get_data(x, y):\n",
    "  return x\n",
    "\n",
    "# Use the map() function to extract the labels\n",
    "dataset_tmp = test_auto.map(get_data)\n",
    "dataset_tmp_sub = test_auto_sub.map(get_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9868826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions for cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 19:44:46.424808: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype resource\n",
      "\t [[{{node Placeholder/_9}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions: 22.72735857963562 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 19:45:09.144592: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype resource\n",
      "\t [[{{node Placeholder/_9}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sub predictions: 21.249239444732666 seconds\n",
      "Suma trenowalnych parametrów: 192545\n",
      "Suma trenowalnych parametrów sub: 160161\n",
      "Running predictions for transformer\n",
      "Model predictions: 22.944324016571045 seconds\n",
      "Model sub predictions: 22.16677451133728 seconds\n",
      "Suma trenowalnych parametrów: 71777\n",
      "Suma trenowalnych parametrów sub: 67681\n",
      "Running predictions for lstm\n",
      "Model predictions: 49.38928747177124 seconds\n",
      "Model sub predictions: 32.75259304046631 seconds\n",
      "Suma trenowalnych parametrów: 620321\n",
      "Suma trenowalnych parametrów sub: 604321\n",
      "Running predictions for autoencoder\n",
      "Model predictions: 22.603391885757446 seconds\n",
      "Model sub predictions: 21.303611516952515 seconds\n",
      "Suma trenowalnych parametrów: 148288\n",
      "Suma trenowalnych parametrów sub: 37280\n",
      "Running predictions for svm\n",
      "Model predictions: 2187.1206471920013 seconds\n",
      "Model sub predictions: 1154.9324202537537 seconds\n"
     ]
    }
   ],
   "source": [
    "#run and time predictions\n",
    "def predict_and_time(model,model_sub,name):\n",
    "    print(f\"Running predictions for {name}\")\n",
    "    start_time = time.time()\n",
    "    reconstructions = model.predict(test_auto, verbose=0)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Model predictions: {elapsed_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    reconstructions_sub = model_sub.predict(test_auto_sub, verbose=0)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Model sub predictions: {elapsed_time} seconds\")\n",
    "    trainable_params = int(np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]))\n",
    "    print(\"Suma trenowalnych parametrów:\", trainable_params)\n",
    "    trainable_params = int(np.sum([tf.keras.backend.count_params(w) for w in model_sub.trainable_weights]))\n",
    "    print(\"Suma trenowalnych parametrów sub:\", trainable_params)\n",
    "    return reconstructions, reconstructions_sub\n",
    "def predict_and_time_svm(model,model_sub):\n",
    "    print(f\"Running predictions for svm\")\n",
    "    start_time = time.time()\n",
    "    reconstructions = model.predict(list(test_text))\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Model predictions: {elapsed_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    reconstructions_sub = model_sub.predict(list(test_text_sub))\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Model sub predictions: {elapsed_time} seconds\")\n",
    "    return reconstructions, reconstructions_sub    \n",
    "\n",
    "reconstructions_cnn, reconstructions_sub_cnn = predict_and_time(cnn_model,cnn_model_sub,\"cnn\")\n",
    "reconstructions_trans, reconstructions_sub_trans = predict_and_time(transformer_model,transformer_model_sub,\"transformer\")\n",
    "reconstructions_lstm, reconstructions_sub_lstm = predict_and_time(lstm_model,lstm_model_sub,\"lstm\")\n",
    "reconstructions_auto, reconstructions_sub_auto = predict_and_time(autoencoder_model,autoencoder_model_sub,\"autoencoder\")\n",
    "reconstructions_svm, reconstructions_sub_svm = predict_and_time_svm(svm_model,svm_model_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05b15cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the lists to disk\n",
    "with open('reconstructions_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_cnn, f)\n",
    "\n",
    "with open('reconstructions_sub_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_sub_cnn, f)\n",
    "\n",
    "with open('reconstructions_trans.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_trans, f)\n",
    "\n",
    "with open('reconstructions_sub_trans.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_sub_trans, f)\n",
    "\n",
    "with open('reconstructions_lstm.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_lstm, f)\n",
    "\n",
    "with open('reconstructions_sub_lstm.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_sub_lstm, f)\n",
    "\n",
    "with open('reconstructions_auto.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_auto, f)\n",
    "\n",
    "with open('reconstructions_sub_auto.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_sub_auto, f)\n",
    "\n",
    "with open('reconstructions_svm.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_svm, f)\n",
    "\n",
    "with open('reconstructions_sub_svm.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructions_sub_svm, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c01d3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the lists from disk\n",
    "with open('reconstructions_cnn.pkl', 'rb') as f:\n",
    "    reconstructions_cnn = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_sub_cnn.pkl', 'rb') as f:\n",
    "    reconstructions_sub_cnn = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_trans.pkl', 'rb') as f:\n",
    "    reconstructions_trans = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_sub_trans.pkl', 'rb') as f:\n",
    "    reconstructions_sub_trans = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_lstm.pkl', 'rb') as f:\n",
    "    reconstructions_lstm = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_sub_lstm.pkl', 'rb') as f:\n",
    "    reconstructions_sub_lstm = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_auto.pkl', 'rb') as f:\n",
    "    reconstructions_auto = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_sub_auto.pkl', 'rb') as f:\n",
    "    reconstructions_sub_auto = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_svm.pkl', 'rb') as f:\n",
    "    reconstructions_svm = pickle.load(f)\n",
    "\n",
    "with open('reconstructions_sub_svm.pkl', 'rb') as f:\n",
    "    reconstructions_sub_svm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073354ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for model cnn\n",
      "Accuracy: 0.8062578987099127\n",
      "Precision: 0.9998897134124246\n",
      "Recall: 0.7353967021634086\n",
      "F1 Score: 0.8474861454229818\n",
      "MCC: 0.6531883089612455\n",
      "Metrics for model cnn_sub\n",
      "Accuracy: 0.9062078558766402\n",
      "Precision: 0.9981725968987115\n",
      "Recall: 0.8734631919257465\n",
      "F1 Score: 0.9316631235477332\n",
      "MCC: 0.8022341358959175\n",
      "Metrics for model trans\n",
      "Accuracy: 0.8861143860422904\n",
      "Precision: 0.9969991270187691\n",
      "Recall: 0.8469623054728328\n",
      "F1 Score: 0.9158767252883574\n",
      "MCC: 0.7672452245954393\n",
      "Metrics for model trans_sub\n",
      "Accuracy: 0.9179212715968753\n",
      "Precision: 0.9979852337128893\n",
      "Recall: 0.8896626843879999\n",
      "F1 Score: 0.9407159179322555\n",
      "MCC: 0.8230005828802239\n",
      "Metrics for model lstm\n",
      "Accuracy: 0.8226532879837827\n",
      "Precision: 0.9998624021526419\n",
      "Recall: 0.7578187464512914\n",
      "F1 Score: 0.8621751138708167\n",
      "MCC: 0.6751350545255171\n",
      "Metrics for model lstm_sub\n",
      "Accuracy: 0.9197618300409672\n",
      "Precision: 0.9981200326725356\n",
      "Recall: 0.8920613216839129\n",
      "F1 Score: 0.942115182220917\n",
      "MCC: 0.8264607369819451\n"
     ]
    }
   ],
   "source": [
    "def calc_and_print_metrics(y_test,reconsturctions,name):\n",
    "    \n",
    "    if(name.find(\"auto\") != -1):\n",
    "        if(name.find(\"sub\") != -1):\n",
    "            tmp_sub = np.concatenate(list(dataset_tmp_sub.as_numpy_iterator()))\n",
    "            mse = tf.keras.losses.mean_squared_error(tmp_sub, reconsturctions)\n",
    "        else:  \n",
    "            tmp = np.concatenate(list(dataset_tmp.as_numpy_iterator()))\n",
    "            mse = tf.keras.losses.mean_squared_error(tmp, reconsturctions)\n",
    "        reconstructions = mse\n",
    "        y_pred = list(map(lambda x: 0 if x[0]<0.09 else 1, list(reconsturctions)))\n",
    "    else:\n",
    "        y_pred = list(map(lambda x: 0 if x[0]<0.5 else 1, list(reconsturctions)))\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Metrics for model {name}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"MCC:\", mcc)\n",
    "\n",
    "    # Calculate precision-recall curve\n",
    "    #precision, recall, _ = precision_recall_curve(y_test, reconsturctions)\n",
    "    #plt.plot(recall, precision)\n",
    "    #plt.xlabel('Recall')\n",
    "    #plt.ylabel('Precision')\n",
    "    #plt.title('Precision-Recall Curve')\n",
    "    #plt.show()\n",
    "\n",
    "#actual = test_auto.map(lambda x,y:y)\n",
    "#actual = np.concatenate(list(actual.as_numpy_iterator()))\n",
    "\n",
    "calc_and_print_metrics(actual,reconstructions_cnn,\"cnn\")\n",
    "calc_and_print_metrics(actual,reconstructions_sub_cnn,\"cnn_sub\")\n",
    "calc_and_print_metrics(actual,reconstructions_trans,\"trans\")\n",
    "calc_and_print_metrics(actual,reconstructions_sub_trans,\"trans_sub\")\n",
    "calc_and_print_metrics(actual,reconstructions_lstm,\"lstm\")\n",
    "calc_and_print_metrics(actual,reconstructions_sub_lstm,\"lstm_sub\")\n",
    "calc_and_print_metrics(actual,reconstructions_auto,\"auto\")\n",
    "calc_and_print_metrics(actual,reconstructions_sub_auto,\"auto_sub\")\n",
    "calc_and_print_metrics(actual,reconstructions_svm,\"svm\")\n",
    "calc_and_print_metrics(actual,reconstructions_sub_svm,\"svm_sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics for model cnn\n",
    "Accuracy: 0.79481590174641\n",
    "Precision: 0.999887318502302\n",
    "Recall: 0.7197650030707192\n",
    "F1 Score: 0.8370109351102607\n",
    "MCC: 0.6383416431313791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2db988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfd905c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117899"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d02c38b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25065"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(malicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6294b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
