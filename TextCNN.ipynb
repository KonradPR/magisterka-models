{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1271dc",
   "metadata": {},
   "source": [
    "<h4>TextCNN</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecff269",
   "metadata": {},
   "source": [
    "<h5>Imports</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "470aa940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse, unquote_plus\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Input, Dropout\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "import jsonlines\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d237b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=['malicious','normal']\n",
    "for name in file_names:\n",
    "    data=[]\n",
    "    with jsonlines.open('dataset/'+name+'.txt') as reader:\n",
    "        for line in reader:\n",
    "            if line['request']['method'] == 'POST':\n",
    "                data.append({'request':{'method':'POST', 'uri':line['request']['uri'], 'body':line['request']['body'],'headers':line['request']['headers']},'metadata':line['metdata']})\n",
    "            else:\n",
    "                data.append({'request':{'method':'GET', 'uri':line['request']['uri'], 'headers':line['request']['headers']},'metadata':line['metdata']})\n",
    "    with jsonlines.open('dataset/'+name+'_clean.txt', mode='w') as writer:\n",
    "        writer.write_all(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e85fe",
   "metadata": {},
   "source": [
    "<h4>Load and create datasets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "debacec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'a' b'/' b'a' b'/' b'a' b'/' b'a' b'{' b'\"a\"' b'=' b'\"a\"' b'}' b'{'\n",
      " b'\"a-a\":' b'\"a\",' b'\"a\":' b'\"a' b'/' b'a,*' b'/' b'*\",' b'\"a-a\":' b'\"a,'\n",
      " b'a,' b'a\",' b'\"a-a\":' b'\"a' b'/' b'n.n' b'(' b'a' b'a' b'n.n;' b'x;'\n",
      " b'x' b')' b'a' b'/' b'n.n' b'(' b'a,' b'a' b'a' b')' b'a' b'/' b'n.n.n.n'\n",
      " b'a' b'/' b'n.n\",' b'\"a\":' b'\"a' b'=' b'x;' b'a' b'=' b'x\"' b'}'] 0\n",
      "[b'post' b'/' b'rest' b'/' b'products' b'/' b'reviews' b'{' b'\"id\"' b'='\n",
      " b'\"ajgoztlb\"' b'}' b'{' b'\"accept-language\":' b'\"de\",' b'\"accept\":'\n",
      " b'\"text' b'/' b'css,*' b'/' b'*\",' b'\"accept-encoding\":' b'\"gzip,'\n",
      " b'deflate,' b'br\",' b'\"user-agent\":' b'\"mozilla' b'/' b'5.0' b'('\n",
      " b'windows' b'nt' b'10.0;' b'win64;' b'x64' b')' b'applewebkit' b'/'\n",
      " b'537.36' b'(' b'khtml,' b'like' b'gecko' b')' b'chrome' b'/'\n",
      " b'72.0.3626.121' b'safari' b'/' b'537.36\",' b'\"cookie\":' b'\"phpsessid'\n",
      " b'=' b'33ptq3esqvyehncdmubiygychvpzr7gw;' b'continuecode' b'='\n",
      " b'8dywbdjgirzm76ncpx2hf8kbrclgrd76\"' b'}'] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 12:54:15.094414: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-22 12:54:15.156692: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "file_names = ['dataset/malicious_clean.txt' ,'dataset/normal_clean.txt']\n",
    "data={}\n",
    "for file in file_names:\n",
    "    data[file]=[]\n",
    "    with jsonlines.open(file) as reader:\n",
    "        for line in reader:\n",
    "            if line['request']['method'] == 'POST':\n",
    "                data[file].append('POST'+' '+line['request']['uri']+' '+line['request']['body']+' '+json.dumps(line['request']['headers']))\n",
    "            else:\n",
    "                data[file].append('GET'+' '+line['request']['uri']+' '+json.dumps(line['request']['headers']))\n",
    "                \n",
    "                \n",
    "normal = data[file_names[1]]\n",
    "malicious = data[file_names[0]]\n",
    "normal_part1 = normal[180000:]\n",
    "normal_part2 = normal[:180000]\n",
    "\n",
    "train_examples = normal_part2\n",
    "test_examples = normal_part1+malicious\n",
    "train_labels = [0] * len(train_examples)\n",
    "test_labels = [0]* len(normal_part1)\n",
    "test_labels.extend([1] * len(malicious))\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((list(map(lambda x: unquote_plus(x),train_examples)), train_labels))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((list(map(lambda x: unquote_plus(x),test_examples)), test_labels))\n",
    "\n",
    "\n",
    "def preprocess_text(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    punctuation = \"=?/(){}[]<>\"\n",
    "    for p in punctuation:\n",
    "        text = tf.strings.regex_replace(text, \"\\\\\" + p, \" \"+p+\" \")\n",
    "    text = tf.strings.split(text)\n",
    "    return text, label\n",
    "\n",
    "def preprocess_text_substitution(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[a-zA-Z]+\", \"a\")\n",
    "    text = tf.strings.regex_replace(text, \"[0-9]+\", \"n\")\n",
    "    text = tf.strings.regex_replace(text, \"(a|n){2,}\", \"x\")\n",
    "    punctuation = \"=?/(){}[]<>\"\n",
    "    for p in punctuation:\n",
    "        text = tf.strings.regex_replace(text, \"\\\\\" + p, \" \"+p+\" \")\n",
    "    text = tf.strings.split(text)\n",
    "    return text, label\n",
    "\n",
    "\n",
    "# Map the preprocess function to the dataset\n",
    "train = dataset_train.map(preprocess_text)\n",
    "test = dataset_test.map(preprocess_text)\n",
    "\n",
    "train_sub = dataset_train.map(preprocess_text_substitution)\n",
    "test_sub = dataset_test.map(preprocess_text_substitution)\n",
    "\n",
    "for text, label in train_sub.take(1):\n",
    "    print(text.numpy(), label.numpy())\n",
    "\n",
    "for text, label in train.take(1):\n",
    "    print(text.numpy(), label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96917ea5",
   "metadata": {},
   "source": [
    "<h4>Vocabularies</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "843b403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 09:56:42.309485: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-22 10:00:34.952810: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-22 10:04:22.175285: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [117899]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-22 10:06:41.865615: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [117899]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "vectorization = TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=256,\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "\n",
    "vectorization_sub = TextVectorization(\n",
    "    max_tokens=1000, \n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=256,\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "\n",
    "vectorization.adapt(train.map(lambda x, y: x))\n",
    "vectorization_sub.adapt(train_sub.map(lambda x, y: x))\n",
    "vectorization.adapt(test.map(lambda x, y: x))\n",
    "vectorization_sub.adapt(test_sub.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081820b3",
   "metadata": {},
   "source": [
    "<h4>Autoencoder</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5804c52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 13:05:22.147547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"global_average_pooling1d_3\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train the autoencoder\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_auto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedfhk2z9_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/konpr/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"global_average_pooling1d_3\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24c366f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'a' b'/' b'a' b'/' b'a' b'/' b'a' b'{' b'\"a\"' b'=' b'\"a\"' b'}' b'{'\n",
      " b'\"a-a\":' b'\"a\",' b'\"a\":' b'\"a' b'/' b'a,*' b'/' b'*\",' b'\"a-a\":' b'\"a,'\n",
      " b'a,' b'a\",' b'\"a-a\":' b'\"a' b'/' b'n.n' b'(' b'a' b'a' b'n.n;' b'x;'\n",
      " b'x' b')' b'a' b'/' b'n.n' b'(' b'a,' b'a' b'a' b')' b'a' b'/' b'n.n.n.n'\n",
      " b'a' b'/' b'n.n\",' b'\"a\":' b'\"a' b'=' b'x;' b'a' b'=' b'x\"' b'}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 12:59:34.394929: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [180000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05b15cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01d3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    split=custom_split\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "073354ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../../var/lib/malocate/mlocate.db'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_data('%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fvar/lib/MAlocate/mlocate.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa35e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
